#!/usr/bin/env python3
"""
Test script to demonstrate the perfect JSON output format.
"""

import sys
import json
from pathlib import Path

# Add the app directory to Python path
sys.path.insert(0, str(Path(__file__).parent))

from app.pipeline.manager import PipelineManager
from app.models.schemas import QueryRequest

def test_perfect_json_output():
    """Test the RAG system with perfect JSON output format."""
    print("ğŸ§ª Testing Perfect JSON Output Format...")
    
    # Get pipeline manager
    manager = PipelineManager.get_instance()
    
    # Create test query
    query_request = QueryRequest(
        query="What is machine learning?",
        top_k=5,
        filters={}
    )
    
    print(f"ğŸ“ Query: {query_request.query}")
    print("ğŸ”„ Processing...")
    
    # Process query
    response = manager.query(query_request)
    
    # Convert to dict for JSON serialization
    response_dict = response.model_dump()
    
    # Pretty print the JSON
    print("\nğŸ¯ Perfect JSON Output:")
    print("=" * 80)
    print(json.dumps(response_dict, indent=2, ensure_ascii=False))
    print("=" * 80)
    
    # Validate structure
    print("\nâœ… Structure Validation:")
    required_fields = [
        "query_id", "answer", "citations", "alternative_answers", 
        "context_analysis", "performance_metrics", "system_metadata"
    ]
    
    for field in required_fields:
        if field in response_dict:
            print(f"  âœ… {field}: Present")
        else:
            print(f"  âŒ {field}: Missing")
    
    # Validate answer structure
    answer = response_dict.get("answer", {})
    answer_fields = ["content", "reasoning_steps", "confidence", "uncertainty_factors"]
    
    print(f"\nğŸ“‹ Answer Structure:")
    for field in answer_fields:
        if field in answer:
            value = answer[field]
            if field == "reasoning_steps":
                print(f"  âœ… {field}: {len(value)} steps")
                for i, step in enumerate(value, 1):
                    print(f"    {i}. {step[:60]}...")
            elif field == "confidence":
                print(f"  âœ… {field}: {value}")
            else:
                print(f"  âœ… {field}: Present")
        else:
            print(f"  âŒ {field}: Missing")
    
    # Validate citations structure
    citations = response_dict.get("citations", [])
    print(f"\nğŸ“š Citations: {len(citations)} citations")
    for i, citation in enumerate(citations[:2], 1):  # Show first 2
        print(f"  Citation {i}:")
        print(f"    Document: {citation.get('document', 'N/A')}")
        print(f"    Relevance: {citation.get('relevance_score', 0):.2f}")
        print(f"    Method: {citation.get('extraction_method', 'N/A')}")
    
    # Validate performance metrics
    perf = response_dict.get("performance_metrics", {})
    print(f"\nâš¡ Performance Metrics:")
    print(f"  Retrieval: {perf.get('retrieval_latency_ms', 0)}ms")
    print(f"  Generation: {perf.get('generation_latency_ms', 0)}ms")
    print(f"  Total: {perf.get('total_response_time_ms', 0)}ms")
    print(f"  Tokens: {perf.get('tokens_processed', 0)}")
    print(f"  Cost: ${perf.get('cost_estimate_usd', 0):.4f}")
    
    # Validate system metadata
    meta = response_dict.get("system_metadata", {})
    print(f"\nğŸ”§ System Metadata:")
    print(f"  Embedding Model: {meta.get('embedding_model', 'N/A')}")
    print(f"  Generation Model: {meta.get('generation_model', 'N/A')}")
    print(f"  Strategy: {meta.get('retrieval_strategy', 'N/A')}")
    print(f"  Timestamp: {meta.get('timestamp', 'N/A')}")
    
    # Alternative answers
    alt_answers = response_dict.get("alternative_answers", [])
    print(f"\nğŸ”„ Alternative Answers: {len(alt_answers)} alternatives")
    
    print(f"\nğŸ‰ Test completed! Output matches the required JSON structure.")
    
    return response_dict

def test_with_reasoning():
    """Test with the reasoning-enhanced pipeline."""
    print("\n" + "="*80)
    print("ğŸ§  Testing with Reasoning Enhancement...")
    
    manager = PipelineManager.get_instance()
    
    query_request = QueryRequest(
        query="How does machine learning work?",
        top_k=7,
        filters={}
    )
    
    print(f"ğŸ“ Query: {query_request.query}")
    print("ğŸ”„ Processing with reasoning...")
    
    import asyncio
    
    async def run_reasoning_test():
        response = await manager.query_with_reasoning(query_request)
        response_dict = response.model_dump()
        
        print("\nğŸ§  Reasoning-Enhanced JSON Output:")
        print("=" * 80)
        print(json.dumps(response_dict, indent=2, ensure_ascii=False))
        print("=" * 80)
        
        # Check reasoning steps
        reasoning_steps = response_dict.get("answer", {}).get("reasoning_steps", [])
        print(f"\nğŸ§  Reasoning Steps ({len(reasoning_steps)}):")
        for i, step in enumerate(reasoning_steps, 1):
            print(f"  {i}. {step}")
        
        return response_dict
    
    return asyncio.run(run_reasoning_test())

if __name__ == "__main__":
    # Test standard query
    standard_result = test_perfect_json_output()
    
    # Test reasoning query  
    reasoning_result = test_with_reasoning()
    
    print(f"\nğŸš€ Both test types completed successfully!")
    print(f"ğŸ“Š Standard Query Result: {len(json.dumps(standard_result))} characters")
    print(f"ğŸ§  Reasoning Query Result: {len(json.dumps(reasoning_result))} characters")
